# SKN07-3rd-3Team

# ğŸ“˜ RAG ê¸°ë°˜ ì˜ì–´ í•™ìŠµ ì±—ë´‡
 
## ğŸ‘¥ íŒ€ ì†Œê°œ
| <img src="https://github.com/user-attachments/assets/b5a17c3c-8415-409b-ae90-0a931e677fc3" width="250" height="250"/> | <img src="https://github.com/user-attachments/assets/005f1a53-0700-420e-8c62-ae1555dd538b" width="250" height="260"/> | <img src="https://github.com/user-attachments/assets/3009a31a-d5ab-469a-bf39-39e6c7779efe" width="250" height="250"/>  | 
|:----------:|:----------:|:----------:|
| ì˜ì–´ ì„ ìƒë‹˜ | êµ­ì–´ ì„ ìƒë‹˜ | ì‚¬íšŒ ì„ ìƒë‹˜ | 
| ì„œì£¼í˜ | ëŒ€ì„±ì› | ìœ¤ì •ì—° | 

<br>

---

## ğŸ“– í”„ë¡œì íŠ¸ ê°œìš”
- í”„ë¡œì íŠ¸ ëª…:  RAG ê¸°ë°˜ ì˜ì–´ í•™ìŠµ ì±—ë´‡
- í”„ë¡œì íŠ¸ ì†Œê°œ: AI ê³ ì¡¸ ê²€ì •ê³ ì‹œ í•™ìŠµ íŠœí„°ëŠ” ê²€ì •ê³ ì‹œë¥¼ ì¤€ë¹„í•˜ëŠ” í•™ìŠµìë¥¼ ìœ„í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. 2018ë…„ë¶€í„° 2024ë…„ê¹Œì§€ 7ë…„ì¹˜ì˜ ê³ ì¡¸ ê²€ì •ê³ ì‹œ ê¸°ì¶œ ë¬¸ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©° ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ì •ë‹µê³¼ í•´ì„¤ì„ ì œê³µí•©ë‹ˆë‹¤.
  ì´ í”„ë¡œì íŠ¸ëŠ” **Retrieval-Augmented Generation (RAG)** ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ì˜ì–´ í•™ìŠµì„ ë„ì™€ì£¼ëŠ” **Streamlit ê¸°ë°˜ ì±—ë´‡**ì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ì¼ë°˜ ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ FAISS ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëœë¤ìœ¼ë¡œ ì¶œì œë˜ëŠ” ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  
- í”„ë¡œì íŠ¸ í•„ìš”ì„±(ë°°ê²½):
  - ê¸°ì¡´ì˜ ê²€ì •ê³ ì‹œ í•™ìŠµ ìë£ŒëŠ” ê°œë³„ì ì¸ ì§ˆë¬¸ì— ëŒ€í•œ ì¦‰ê°ì ì¸ ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
  - ì‹¤ì œ ê¸°ì¶œëœ ë¬¸ì œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë¯€ë¡œ í•™ìŠµìëŠ” ê²€ì •ê³ ì‹œ ëŒ€ë¹„ì— ìˆì–´ ë³´ë‹¤ ì²´ê³„ì ì´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  - ê²½ì œì , í™˜ê²½ì  ìš”ì¸ìœ¼ë¡œ ì¸í•´ ì •ê·œ êµìœ¡ì„ ë°›ê¸° ì–´ë ¤ìš´ í•™ìŠµìë“¤ì—ê²Œ ê³µìµì ì¸ í•™ìŠµ ê¸°íšŒë¥¼ ì œê³µí•˜ë©°, êµìœ¡ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ë° ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- í”„ë¡œì íŠ¸ ëª©í‘œ:
  ë³¸ í”„ë¡œì íŠ¸ëŠ” AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ í•™ìŠµìê°€ ê²€ì •ê³ ì‹œë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ì¤€ë¹„í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
  - Q&A ì„œë¹„ìŠ¤ ê°œë°œ: 2018ë…„ë¶€í„° 2024ë…„ê¹Œì§€ì˜ ê³ ì¡¸ ê²€ì •ê³ ì‹œ ê¸°ì¶œë¬¸ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ CHAT GPT APIë¥¼ í™œìš©í•˜ì—¬ AIê°€ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
  - ìì—°ì–´ ì²˜ë¦¬(NLP) ë° LLM(Chat GPT API) í™œìš©: í…ìŠ¤íŠ¸ ë¶„ì„ê³¼ ë¬¸ì„œ ì´í•´ ê¸°ìˆ ì„ ì ìš©í•˜ì—¬ ì§ˆì˜ì‘ë‹µ ì •í™•ë„ë¥¼ í–¥ìƒí•©ë‹ˆë‹¤.
  - ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤ ì œê³µ: í•™ìŠµìê°€ ì‰½ê²Œ ì ‘ê·¼í•˜ê³  í™œìš©í•  ìˆ˜ ìˆëŠ” ì§ê´€ì ì¸ í™˜ê²½ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.
  - í•™ìŠµ íš¨ê³¼ ì¦ëŒ€: AIë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ì‘ë‹µ ì‹œìŠ¤í…œìœ¼ë¡œ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ì§€ì›í•©ë‹ˆë‹¤.
  - ë§ì¶¤í˜• í•™ìŠµ ì œê³µ: í•™ìŠµì´ í•„ìš”í•œ ë¬¸ì œ ìœ í˜•ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë¬¸ì œì™€ ë‹µ, í•´ì„¤ì„ ì œê³µí•˜ì—¬ í•™ìŠµì„ ì¦ì§„í•©ë‹ˆë‹¤.

---

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥ ì†Œê°œ

1. **ì¼ë°˜ ì§ˆë¬¸ ì‘ë‹µ**
   - ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì— ëŒ€í•´ OpenAI GPT-3.5ê°€ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
2. **ëœë¤ ë¬¸ì œ í’€ê¸°**
   - FAISS ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì˜ì–´ ë¬¸ì œë¥¼ ëœë¤ìœ¼ë¡œ ë¶ˆëŸ¬ì™€ ì‚¬ìš©ìê°€ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
   - ì„ íƒì§€ë¥¼ ì œê³µí•˜ë©° ì •ë‹µì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

<img src="https://github.com/user-attachments/assets/81339928-6c06-4bf1-8871-71630856ecac" alt="í”„ë¡œì íŠ¸ êµ¬ì¡°" width="800px">

---

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ



---

## ğŸ“‘ ì£¼ìš” í”„ë¡œì‹œì €

### â–¶ï¸ 1. ë°ì´í„° ìˆ˜ì§‘ _ ì›¹ í¬ë¡¤ë§ (í•œêµ­êµìœ¡ê³¼ì •í‰ê°€ì›)
- ë©”ì¸ í˜ì´ì§€ì—ì„œ ê° ì—°ë„, í•™ë ¥ë¶„ë¥˜, ì°¨ìˆ˜, ê³¼ëª©ë³„ ì½”ë“œë²ˆí˜¸ ì¶”ì¶œ
```python
code_dict = {}
tmp = 1
while True:   # ëª¨ë“  í˜ì´ì§€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    url = f'https://www.kice.re.kr/boardCnts/list.do?type=default&page={tmp}&selLimitYearYn=Y&selStartYear=2018&C06=&boardID=1500211&C05=&C04=&C03=&searchType=S&C02=&C01='
    re = requests.get(url)
    soup = BeautifulSoup(re.text, 'html.parser')
    
    if soup.find('td').text.strip() == 'ë“±ë¡ëœ ê²Œì‹œë¬¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.':
        break
        
    info = soup.find('tbody').find_all('tr')
    
    for i in info:
        code = i.find_all('td')[0].text
        year = i.find_all('td')[1].text
        edu = i.find_all('td')[2].text
        cnt = i.find_all('td')[3].text
        subject = i.find_all('td')[4].find('a')['title']
        # ì›í•˜ëŠ” ìë£Œ ì •ë³´ ì„ íƒ
        if edu == 'ê³ ì¡¸í•™ë ¥' and subject == 'ì˜ì–´':
            code_dict[code] = f'{year}_{edu}_{cnt}_{subject}'
    
    tmp += 1
```
![image](https://github.com/user-attachments/assets/be19cd7c-60ca-4ed9-a431-04b07a2ace09)
- ì¶”ì¶œëœ ì½”ë“œë²ˆí˜¸ë¡œ ì„¸ë¶€ í˜ì´ì§€ ì ‘ì† í›„ PDF íŒŒì¼ ì €ì¥
```python
for code in code_dict.keys():
    down_url = f'https://www.kice.re.kr/boardCnts/view.do?boardID=1500211&boardSeq={code}&lev=0&m=030305&searchType=S&statusYN=W&page=1&s=kice'
    down_re = requests.get(down_url)
    down_soup = BeautifulSoup(down_re.text)
    tmp_url = down_soup.find(class_='fieldBox').find('a')['href']
    pdf_url = 'https://www.kice.re.kr' + tmp_url
    file_path = f'./data/ì •ë‹µ/{code_dict[code]}.pdf'
    
    response = requests.get(pdf_url)
    # ì‘ë‹µ ìƒíƒœ ì½”ë“œê°€ 200(ì„±ê³µ)ì¸ ê²½ìš°ì—ë§Œ íŒŒì¼ ì €ì¥
    if response.status_code == 200:
        with open(file_path, 'wb') as file:
            file.write(response.content)  # PDF íŒŒì¼ ì €ì¥
```
### â–¶ï¸ 2. ë°ì´í„° ì¶”ì¶œ
### 2-1) ê³ ë“± ì˜ì–´ ë¬¸ì œ ì¶”ì¶œ

- PDFì—ì„œ ì™¼ìª½/ì˜¤ë¥¸ìª½ ë¬¸í•­ì„ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì •ë¦¬í•˜ì—¬ ì¶”ì¶œ
```python
def extract_text_from_pdf(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        combined_text_list = []

        for page in pdf.pages:
            width, height = page.width, page.height

            # ì™¼ìª½ ë¬¸í•­ (í˜ì´ì§€ë³„ ì™¼ìª½ ë¨¼ì €)
            left_bbox = (0, 0, width / 2, height)
            left_crop = page.within_bbox(left_bbox)
            left_text = left_crop.extract_text()
            if left_text:
                combined_text_list.append(clean_text(left_text))  # ì •ë¦¬ í›„ ì¶”ê°€

            # ì˜¤ë¥¸ìª½ ë¬¸í•­ (í˜ì´ì§€ë³„ ì˜¤ë¥¸ìª½ ë‚˜ì¤‘)
            right_bbox = (width / 2, 0, width, height)
            right_crop = page.within_bbox(right_bbox)
            right_text = right_crop.extract_text()
            if right_text:
                combined_text_list.append(clean_text(right_text))
```

- OCR ì´ë¯¸ì§€ PDF ì²˜ë¦¬ (í˜ì´ì§€ë³„ ì™¼ìª½ â†’ ì˜¤ë¥¸ìª½)
```python
def extract_text_from_image_pdf(pdf_path):
    images = convert_from_path(pdf_path, dpi=300)
    combined_text_list = []

    for img in images:
        width, height = img.size

        # ì™¼ìª½ ë¬¸í•­ OCR
        left_crop = img.crop((0, 0, width // 2, height))
        left_text = pytesseract.image_to_string(left_crop, lang="eng+kor", config="--psm 6")
        combined_text_list.append(clean_text(left_text))

        # ì˜¤ë¥¸ìª½ ë¬¸í•­ OCR
        right_crop = img.crop((width // 2, 0, width, height))
        right_text = pytesseract.image_to_string(right_crop, lang="eng+kor", config="--psm 6")
        combined_text_list.append(clean_text(right_text))

    return "\n".join(combined_text_list)
```

### 2-2) ê³ ë“± ì˜ì–´ ì •ë‹µ ì¶”ì¶œ
- PDFì—ì„œ ì˜ì–´ ì •ë‹µ ì¶”ì¶œ (OCR í¬í•¨)
```python
def extract_english_answers_from_pdf(pdf_path):
    answers = {}

    # 1ï¸âƒ£ PDFì—ì„œ ì§ì ‘ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    with pdfplumber.open(pdf_path) as pdf:
        text = "\n".join([page.extract_text() for page in pdf.pages if page.extract_text()])

    # 2ï¸âƒ£ OCR ì ìš© (í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìœ¼ë©´ OCR ì‚¬ìš©)
    if not text.strip():
        text = extract_text_from_image_pdf(pdf_path)

    # 3ï¸âƒ£ OCRë¡œ ì¶”ì¶œí•œ ì›ë³¸ í…ìŠ¤íŠ¸ ì¶œë ¥ (ë””ë²„ê¹… ëª©ì )
    print("\nğŸ“ OCR EXTRACTED TEXT FROM PDF:", pdf_path)
    print(text[:1000])  # ì²˜ìŒ 1000ìë§Œ ì¶œë ¥

    # 4ï¸âƒ£ "ì˜ì–´ ì •ë‹µí‘œ" ë˜ëŠ” "3êµì‹œ ì˜ì–´" í¬í•¨ëœ ë¶€ë¶„ ì°¾ê¸°
    match = re.search(r"(?:ì˜ì–´ ì •ë‹µí‘œ|3êµì‹œ ì˜ì–´|ì˜ì–´)([\s\S]+?)(?=\n\w+ ì •ë‹µí‘œ|\Z)", text)
    if match:
        english_answers_section = match.group(1).strip()
    else:
        print(f"âš  ì˜ì–´ ì •ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pdf_path}")
        return None

    # 5ï¸âƒ£ ì •ë‹µ íŒ¨í„´ ì¶”ì¶œ (ë””ë²„ê¹…ìš© ì¶œë ¥ ì¶”ê°€)
    extracted_text = convert_korean_numbers(english_answers_section)
    print("\nğŸ” EXTRACTED ENGLISH ANSWERS SECTION:")
    print(extracted_text[:500])  # ì²˜ìŒ 500ìë§Œ ì¶œë ¥

    # 6ï¸âƒ£ ë¬¸í•­ë²ˆí˜¸ & ì •ë‹µ ì¶”ì¶œ
    answer_pattern = re.findall(r"(\d+)\s+([â‘ â‘¡â‘¢â‘£1-4])", extracted_text)

    # ğŸ”¥ ë””ë²„ê¹…: ì¶”ì¶œëœ ì •ë‹µ ì¶œë ¥
    print("\nğŸ” Extracted Answers Dictionary:", answer_pattern)

    for q_num, ans in answer_pattern:
        answers[q_num] = ans

    return answers
```

### 2-3) ê³ ë“± ì˜ì–´ ë¬¸ì œ, ì •ë‹µ jsoníŒŒì¼ í•©ì¹˜ê¸°
- íŒŒì¼ëª… ì •ë¦¬ (ì •ë‹µ íŒŒì¼ëª…ê³¼ ë¬¸ì œ íŒŒì¼ëª… ì¼ì¹˜í•˜ë„ë¡ ë³€í™˜)
```python
def clean_filename(filename):
    return filename.replace("_ê³ ë“±_ì •ë‹µ.pdf", "_ê³ ë“±_ì˜ì–´.pdf")  # ì •ë‹µ íŒŒì¼ëª…ì„ ë¬¸ì œ íŒŒì¼ëª…ê³¼ ë§ì¶¤
```

- ë³€í™˜ëœ ì •ë‹µ ë°ì´í„° í‚¤ ê°’ ìˆ˜ì •
```python
answers_data_fixed = {clean_filename(k): v for k, v in answers_data.items()}\
```

- ë¬¸ì œì™€ ì •ë‹µ ë§¤ì¹­
```python
merged_data = {}

for file_name, question_content in questions_data.items():
    matched_file = clean_filename(file_name)
    if matched_file in answers_data_fixed:  # ì •ë‹µì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
        merged_data[file_name] = {
            "questions": question_content,
            "answers": answers_data_fixed[matched_file]
        }
    else:
        print(f"âš  ì •ë‹µì´ ì—†ëŠ” ë¬¸ì œ íŒŒì¼: {file_name}")
```

### â–¶ï¸ 3. ì„ë² ë”©
- OpenAIì˜ "text-embedding-ada-002" ëª¨ë¸ë¥¼ ì‚¬ìš©í•´ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
```
# OpenAI ì„ë² ë”© í•¨ìˆ˜ (ìµœì‹  API ì ìš©)
def get_embedding(text):
    response = openai.embeddings.create(
        input=text,
        model="text-embedding-ada-002"
    )
    return response.data[0].embedding  # ìµœì‹  API ë°©ì‹ ì ìš©

# ëª¨ë“  ì§ˆë¬¸ì„ ì„ë² ë”© ë³€í™˜
question_embeddings = [get_embedding(q) for q in questions]
```

- FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
```
embedding_dim = 1536  # ë²¡í„° ì°¨ì› ì„¤ì •
index = faiss.IndexFlatL2(embedding_dim)  # FAISS ì¸ë±ìŠ¤ ìƒì„±
question_vectors = np.array(question_embeddings).astype("float32")  # ì„ë² ë”© ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
index.add(question_vectors)  # FAISS ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€
```

- FAISSë¥¼ ì´ìš©í•œ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰
```
def search_similar_questions(query, top_k=3):
    # ì…ë ¥ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    query_vector = np.array(get_embedding(query)).astype("float32").reshape(1, -1)

    # ê°€ì¥ ê°€ê¹Œìš´ ì§ˆë¬¸ ê²€ìƒ‰
    distances, indices = index.search(query_vector, top_k)

    # ê²°ê³¼ ì¶œë ¥
    print("\n[ê°€ì¥ ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤]")
    for i in range(top_k):
        idx = indices[0][i]
        print(f"{i+1}. {questions[idx]} (ê±°ë¦¬: {distances[0][i]:.4f})")
```

- FAISS ì¸ë±ìŠ¤ ì €ì¥
```
faiss.write_index(index, "faiss_index.bin")
```
---

## ğŸ¬ìˆ˜í–‰ê²°ê³¼(í…ŒìŠ¤íŠ¸/ì‹œì—° í˜ì´ì§€)

---
 
## ğŸ’­í•œ ì¤„ íšŒê³ 
